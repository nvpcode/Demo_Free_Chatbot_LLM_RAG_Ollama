import streamlit as st
import os
import json
import shutil
from langchain_community.vectorstores import FAISS
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

from chunks_data.split_chunks import chunk_documents
from chunks_data.load_data import  load_data
from models.embedder import get_embedding_model
from retriever.retriever import get_hybrid_retriever
from rerank.reranker import get_rerank_from_retriever
from models.llm import get_answer_from_llm
from get_info_chat_history import get_info
# from create_suggestions.suggestions import generate_suggestions_from_docs

from cache_communicate.caching_communicate import CacheManager


# Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng
model_LLM = "incept5/llama3.1-claude:latest"


# ----------------------------
# Setup Page
# ----------------------------
def setup_page():
    st.set_page_config(
        page_title="ChatBot_RAG",
        page_icon="üí¨",
        layout="wide"
    )


# ----------------------------
# Sidebar Config
# ----------------------------
def setup_sidebar():
    st.sidebar.header("1. T·∫£i d·ªØ li·ªáu")
    uploaded_file = st.sidebar.file_uploader(
        "Ch·ªçn file d·ªØ li·ªáu (.txt, .pdf, .docx, .xlsx)",
        type=["txt", "pdf", "docx", "xlsx"],
        accept_multiple_files=True,
    )

    st.sidebar.header("2. Chia chunk")
    chunk_method = st.sidebar.selectbox(
        "Ph∆∞∆°ng ph√°p chia chunk", ["recursive", "semantic"]
    )

    st.sidebar.header("3. AI Model")
    model_choice = st.sidebar.selectbox(
        "Ch·ªçn AI Model ƒë·ªÉ tr·∫£ l·ªùi:", [model_LLM]
    )

    return uploaded_file, chunk_method, model_choice


# ----------------------------
# Initialize app (load, chunk, embed, retriever, LLM)
# ----------------------------
def initialize_app(uploaded_file, chunk_method):
    if not uploaded_file:
        return None, None, None

    # Chunk
    chunk_size, overlap = 1024, 500
    all_chunks = []

    if uploaded_file:
        # N·∫øu th∆∞ m·ª•c t·ªìn t·∫°i th√¨ x√≥a
        if os.path.exists("tmp_data"):
            shutil.rmtree("tmp_data")
        os.makedirs("tmp_data", exist_ok=True)
        for uploaded_file in uploaded_file:
            file_path = os.path.join("tmp_data", uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.read())
            st.success(f"‚úÖƒê√£ l∆∞u file v√†o {file_path}")

            # Load data
            docs = load_data(file_path)
            chunked_docs = chunk_documents(docs, method=chunk_method, chunk_size=chunk_size, overlap=overlap)
            all_chunks.extend(chunked_docs)

    st.info(f"üìë ƒê√£ chia th√†nh {len(all_chunks)} chunk.")

    # Embedding
    embeddings = get_embedding_model()
    vector_store = FAISS.from_documents(all_chunks, embeddings)

    # Retriever
    retriever_obj = get_hybrid_retriever(vector_store, chunked_docs, top_k=3, weights=(0.5, 0.5))

    # Rerank
    rerank_obj = get_rerank_from_retriever(retriever_obj, top_k=3)

    # Kh·ªüi t·∫°o cache manager
    cache_manager = CacheManager()
    cache_manager.set_embedding_model(embeddings)

    return retriever_obj, rerank_obj, cache_manager


# ----------------------------
# Chat interface
# ----------------------------
def setup_chat_interface(model_choice):
    col1, col2 = st.columns([6, 1])
    with col1:
        st.title("üí¨ Chat-NVP")
    with col2:
        if st.button("üîÑ L√†m m·ªõi h·ªôi tho·∫°i"):
            st.session_state.messages = [
                {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?"}
            ]
            st.session_state.input_enabled = True
            st.rerun()

    st.caption(f"üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi {model_choice}")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?"}
        ]
        msgs.add_ai_message("Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n h√¥m nay?")

    # üëâ Hi·ªÉn th·ªã to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

# ----------------------------
# Handle user input
# ----------------------------
def handle_user_input(msgs, retriever_obj, rerank_obj, model_choice, cache_manager):
    if input_user := st.chat_input("H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:"):
        # L∆∞u v√† hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "human", "content": input_user})
        st.chat_message("human").write(input_user)
        msgs.add_user_message(input_user)

        # L·∫•y l·ªãch s·ª≠ chat g·∫ßn nh·∫•t
        max_history_pairs = 5
        history_messages = st.session_state.messages[-(max_history_pairs * 2 + 1):-1]
        # Gh√©p th√†nh chu·ªói "Ng∆∞·ªùi: ...\nBot: ..."
        chat_history_str = "\n".join(
            f"{'Ng∆∞·ªùi d√πng' if m['role'] == 'human' else 'Bot'}: {m['content']}"
            for m in history_messages
        )

        # G·ªçi AI x·ª≠ l√Ω v√† hi·ªÉn th·ªã

        # (Kh√¥ng Streaming)
        # with st.chat_message("assistant"):
        #     response = get_answer(chat_history_str, input_user, retriever, model_choice)

        #     # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
        #     st.session_state.messages.append({"role": "assistant", "content": response})
        #     msgs.add_ai_message(response)
        #     st.write(response)

        # (C√≥ Streaming)
        # S·ª≠ d·ª•ng cache
        with st.chat_message("assistant"):
            placeholder = st.empty()
            
            # Ki·ªÉm tra cache tr∆∞·ªõc
            cached_answer = cache_manager.get_from_cache(input_user)
            
            if cached_answer:
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi t·ª´ cache
                placeholder.markdown(cached_answer)
                streamed_text = cached_answer
            else:
                # G·ªçi LLM n·∫øu kh√¥ng c√≥ trong cache
                streamed_text = ""
                for token in get_answer_from_llm(chat_history_str, input_user, retriever_obj, rerank_obj, model_choice):
                    streamed_text += token
                    placeholder.markdown(streamed_text)
                
                # L∆∞u v√†o cache
                cache_manager.add_to_cache(input_user, streamed_text)
            
            # L∆∞u v√†o session
            st.session_state.messages.append({"role": "assistant", "content": streamed_text})
            msgs.add_ai_message(streamed_text)


# ----------------------------
# Main
# ----------------------------
def main():
    setup_page()
    uploaded_file, chunk_method, model_choice = setup_sidebar()

    # Kh·ªüi t·∫°o app v·ªõi cache
    retriever_obj, rerank_obj, cache_manager = initialize_app(uploaded_file, chunk_method)

    # ‚úÖ Ch·ªâ m·ªü giao di·ªán chat khi d·ªØ li·ªáu v√† agent ƒë√£ s·∫µn s√†ng
    if retriever_obj is not None:
        msgs = setup_chat_interface(model_choice)
        handle_user_input(msgs, retriever_obj, rerank_obj, model_choice, cache_manager)

        # get_info("", retriever_obj, rerank_obj, model_choice)
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i d·ªØ li·ªáu tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán.")
    

if __name__ == "__main__":
    main()
