from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from retriever.retriever import show_retrieved_chunks
from rerank.reranker import show_rerank_chunks

def get_llm(model_choice):
    """
    Tráº£ vá» LLM object cá»§a LangChain
    """
    return ChatOllama(
        model=model_choice,
        temperature=0.001,
        base_url="http://localhost:11434",
        device="cuda"  # Cháº¡y trÃªn gpu
    )

def get_answer_from_llm(chat_history, user_question, retriever_obj, rerank_obj, model_choice):
    # Láº¥y cÃ¡c tÃ i liá»‡u liÃªn quan
    retrieved_docs = show_retrieved_chunks(retriever_obj, user_question)
    reranked_docs = show_rerank_chunks(rerank_obj, user_question)   
    context = "\n".join(doc.page_content for doc in reranked_docs)
    
    # Táº¡o ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages([
    ("system", """Báº¡n lÃ  chuyÃªn gia vá» AI, tráº£ lá»i báº±ng Tiáº¿ng Viá»‡t. TÃªn báº¡n lÃ  NVP-Chatbot.
    Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin cÃ³ trong tÃ i liá»‡u vÃ  lá»‹ch sá»­ há»™i thoáº¡i Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.
    Tuyá»‡t Ä‘á»‘i KHÃ”NG Ä‘Æ°á»£c suy Ä‘oÃ¡n, bá»‹a Ä‘áº·t hoáº·c Ä‘Æ°a thÃ´ng tin ngoÃ i pháº¡m vi tÃ i liá»‡u.
    Náº¿u cÃ¢u há»i khÃ´ng thá»ƒ tráº£ lá»i tá»« cÃ¡c thÃ´ng tin nÃ y, hÃ£y tráº£ lá»i Ä‘Ãºng nguyÃªn vÄƒn: "TÃ´i khÃ´ng cháº¯c cháº¯n".
    Khi tráº£ lá»i, hÃ£y phÃ¢n tÃ­ch vÃ  trÃ¬nh bÃ y rÃµ rÃ ng, cÃ³ thá»ƒ so sÃ¡nh hoáº·c Ä‘Ã¡nh giÃ¡ náº¿u dá»¯ liá»‡u cho phÃ©p.

    --- Lá»‹ch sá»­ há»™i thoáº¡i ---
    {chat_history}

    --- TÃ i liá»‡u há»— trá»£ ---
    {context}
    """
    ),
    ("human", "{question}")
    ])


    # Format prompt vá»›i dá»¯ liá»‡u thá»±c táº¿
    messages = prompt.format_messages(
        chat_history=chat_history,
        context=context,
        question=user_question
    )
    print("ğŸ“ Context length:", len(context), "kÃ½ tá»±\n")

    # Gá»i model
    llm = get_llm(model_choice)

    # DÃ¹ng streaming
    full_response = ""
    for chunk in llm.stream(messages):
        if chunk.content:
            yield chunk.content  # Tráº£ vá» tá»«ng pháº§n text
            full_response += chunk.content

    # KhÃ´ng dÃ¹ng cho streaming
    # response = llm.invoke(messages)
    # return response.content  # Tráº£ vá» toÃ n bá»™ cÃ¢u tráº£ lá»i

    

def get_answer_from_llm_2(chat_history, user_question, retriever_obj, rerank_obj, model_choice):
    # Láº¥y cÃ¡c tÃ i liá»‡u liÃªn quan
    retrieved_docs = show_retrieved_chunks(retriever_obj, user_question)
    reranked_docs = show_rerank_chunks(rerank_obj, user_question)   
    context = "\n".join(doc.page_content for doc in reranked_docs)
    
    # Táº¡o ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages([
    ("system", """Báº¡n lÃ  chuyÃªn gia vá» AI, tráº£ lá»i báº±ng Tiáº¿ng Viá»‡t. TÃªn báº¡n lÃ  NVP-Chatbot.
    Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin cÃ³ trong tÃ i liá»‡u vÃ  lá»‹ch sá»­ há»™i thoáº¡i Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.
    Tuyá»‡t Ä‘á»‘i KHÃ”NG Ä‘Æ°á»£c suy Ä‘oÃ¡n, bá»‹a Ä‘áº·t hoáº·c Ä‘Æ°a thÃ´ng tin ngoÃ i pháº¡m vi tÃ i liá»‡u.
    Náº¿u cÃ¢u há»i khÃ´ng thá»ƒ tráº£ lá»i tá»« cÃ¡c thÃ´ng tin nÃ y, hÃ£y tráº£ lá»i Ä‘Ãºng nguyÃªn vÄƒn: "TÃ´i khÃ´ng cháº¯c cháº¯n".
    Khi tráº£ lá»i, hÃ£y phÃ¢n tÃ­ch vÃ  trÃ¬nh bÃ y rÃµ rÃ ng, cÃ³ thá»ƒ so sÃ¡nh hoáº·c Ä‘Ã¡nh giÃ¡ náº¿u dá»¯ liá»‡u cho phÃ©p.

    --- Lá»‹ch sá»­ há»™i thoáº¡i ---
    {chat_history}

    --- TÃ i liá»‡u há»— trá»£ ---
    {context}
    """
    ),
    ("human", "{question}")
    ])


    # Format prompt vá»›i dá»¯ liá»‡u thá»±c táº¿
    messages = prompt.format_messages(
        chat_history=chat_history,
        context=context,
        question=user_question
    )

    # Gá»i model
    llm = get_llm(model_choice)

    # KhÃ´ng dÃ¹ng cho streaming
    response = llm.invoke(messages)

    return response.content, reranked_docs  
